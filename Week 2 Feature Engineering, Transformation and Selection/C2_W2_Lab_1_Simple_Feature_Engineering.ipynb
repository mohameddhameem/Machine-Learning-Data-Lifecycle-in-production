{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPt5BHTwy_0F"
   },
   "source": [
    "# Ungraded Lab: Simple Feature Engineering\n",
    "\n",
    "In this lab, you will get some hands-on practice with the [Tensorflow Transform](https://www.tensorflow.org/tfx/transform/get_started) library (or `tf.Transform`). This serves to show what is going on under the hood when you get to use the [TFX Transform](https://www.tensorflow.org/tfx/guide/transform) component within a TFX pipeline in the next labs. The code snippets and main discussions are taken from this [TensorFlow official notebook](https://www.tensorflow.org/tfx/tutorials/transform/simple) but we have expounded on a few key points.\n",
    "\n",
    "Preprocessing is often required in ML projects because the raw data is not yet in a suitable format for training a model. Not doing so usually results in the model not converging or having poor performance. Some standard transformations include normalizing pixel values, bucketizing, one-hot encoding, and the like. Consequently, these same transformations should also be done during inference to ensure that the model is computing the correct predictions.\n",
    "\n",
    "With Tensorflow Transform, you can preprocess data using the same code for both training a model and serving inferences in production. It provides several utility functions for common preprocessing tasks including creating features that require a full pass over the training dataset. The outputs are the transformed features and a TensorFlow graph which you can use for both training and serving. Using the same graph for both training and serving can prevent feature skew, since the same transformations are applied in both stages.\n",
    "\n",
    "For this introductory exercise, you will walk through the \"Hello World\" of using TensorFlow Transform to preprocess input data. As you've seen in class, the main steps are to:\n",
    "\n",
    "1. Collect raw data\n",
    "2. Define metadata\n",
    "3. Create a preprocessing function\n",
    "4. Generate a constant graph with the required transformations\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "K4QXVIM7iglN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.6.5\n",
      "TFX Transform version: 1.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "print(f'TFX Transform version: {tft.__version__}')\n",
    "#print(f'TF Beam Transform version: {tft_beam.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxOxaaOYRfl7"
   },
   "source": [
    "## Collect raw data\n",
    "\n",
    "First, you will need to load your data. For simplicity, we will not use a real dataset in this exercise. You will do that in the next lab. For now, you will just use dummy data so you can inspect the transformations more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-R236Tkf_ON3"
   },
   "outputs": [],
   "source": [
    "# define sample data\n",
    "raw_data = [\n",
    "      {'x': 1, 'y': 1, 's': 'hello'},\n",
    "      {'x': 2, 'y': 2, 's': 'world'},\n",
    "      {'x': 3, 'y': 3, 's': 'hello'}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the metadata\n",
    "\n",
    "Next, you will define the metadata. This contains the schema that tells the types of each feature column (or key) in `raw_data`. You need to take note of a few things:\n",
    "\n",
    "* The transform function later expects the metadata to be packed in a [DatasetMetadata](https://github.com/tensorflow/transform/blob/master/tensorflow_transform/tf_metadata/dataset_metadata.py#L23) object. \n",
    "* The constructor for the `DatasetMetadata` class expects a [Schema protocol buffer](https://github.com/tensorflow/metadata/blob/master/tensorflow_metadata/proto/v0/schema.proto#L46) data type. You can use the [schema_from_feature_spec()](https://github.com/tensorflow/transform/blob/master/tensorflow_transform/tf_metadata/schema_utils.py#L36) method to generate that from a dictionary.\n",
    "* To build the said dictionary, you will use the keys/column names of `raw_data` and assign a [FeatureSpecType](https://github.com/tensorflow/transform/blob/master/tensorflow_transform/common_types.py#L29) as values. This allows you to specify if the input is fixed or variable length (using [tf.io](https://www.tensorflow.org/api_docs/python/tf/io) classes), as well as to define the shape and data type.\n",
    "\n",
    "See how this is implemented in the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the schema as a DatasetMetadata object\n",
    "raw_data_metadata = dataset_metadata.DatasetMetadata(\n",
    "    \n",
    "    # use convenience function to build a Schema protobuf\n",
    "    schema_utils.schema_from_feature_spec({\n",
    "        \n",
    "        # define a dictionary mapping the keys to its feature spec type\n",
    "        'y': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'x': tf.io.FixedLenFeature([], tf.float32),\n",
    "        's': tf.io.FixedLenFeature([], tf.string),\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature {\n",
      "  name: \"s\"\n",
      "  type: BYTES\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "  }\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"x\"\n",
      "  type: FLOAT\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "  }\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"y\"\n",
      "  type: FLOAT\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "  }\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preview the schema\n",
    "print(raw_data_metadata._schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zadh6MXLS3eD"
   },
   "source": [
    "## Create a preprocessing function\n",
    "The _preprocessing function_ is the most important concept of `tf.Transform`. A preprocessing function is where the transformation of the dataset really happens. It accepts and returns a dictionary of tensors, where a tensor means a <a target='_blank' href='https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/Tensor'><code>Tensor</code></a> or <a target='_blank' href='https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/SparseTensor'><code>SparseTensor</code></a>. There are two main groups of API calls that typically form the heart of a preprocessing function:\n",
    "\n",
    "1. **TensorFlow Ops:** Any function that accepts and returns tensors. These add TensorFlow operations to the graph that transforms raw data into transformed data one feature vector at a time.  These will run for every example, during both training and serving.\n",
    "2. **TensorFlow Transform Analyzers:** Any of the analyzers provided by `tf.Transform`. Analyzers also accept and return tensors, but unlike TensorFlow ops they only run once during training, and typically make a full pass over the entire training dataset. They create <a target='_blank' href='https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/constant'>tensor constants</a>, which are added to your graph. For example, `tft.min` computes the minimum of a tensor over the training dataset.\n",
    "\n",
    "*Caution: When you apply your preprocessing function to serving inferences, the constants that were created by analyzers during training do not change.  If your data has trend or seasonality components, plan accordingly.*\n",
    "\n",
    "You can see available functions to transform your data [here](https://www.tensorflow.org/tfx/transform/api_docs/python/tft)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "H2wANNF_2dCR"
   },
   "outputs": [],
   "source": [
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
    "    \n",
    "    # extract the columns and assign to local variables\n",
    "    x = inputs['x']\n",
    "    y = inputs['y']\n",
    "    s = inputs['s']\n",
    "    \n",
    "    # data transformations using tft functions\n",
    "    x_centered = x - tft.mean(x)\n",
    "    y_normalized = tft.scale_to_0_1(y)\n",
    "    s_integerized = tft.compute_and_apply_vocabulary(s)\n",
    "    x_centered_times_y_normalized = (x_centered * y_normalized)\n",
    "    \n",
    "    # return the transformed data\n",
    "    return {\n",
    "        'x_centered': x_centered,\n",
    "        'y_normalized': y_normalized,\n",
    "        's_integerized': s_integerized,\n",
    "        'x_centered_times_y_normalized': x_centered_times_y_normalized,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSl9qyTCbBKR"
   },
   "source": [
    "## Generate a constant graph with the required transformations\n",
    "\n",
    "Now you're ready to put everything together and transform your data. Like TFDV last week, Tensorflow Transform also uses [Apache Beam](https://beam.apache.org/) for deployment scalability and flexibility. As you'll see below, Beam uses the pipe (`|`) operator to stack the different stages of the pipeline. In this case, you will just feed the data (and metadata) to the [AnalyzeAndTransformDataset](https://www.tensorflow.org/tfx/transform/api_docs/python/tft_beam/AnalyzeAndTransformDataset) class and use the preprocessing function you defined above to transform the data.\n",
    "\n",
    "For a closer look at Beam syntax for tranform pipelines, you can refer to the documentation [here](https://beam.apache.org/documentation/programming-guide/#applying-transforms) and try the short Colab [here](https://colab.research.google.com/github/apache/beam/blob/master/examples/notebooks/get-started/try-apache-beam-py.ipynb#scrollTo=J5HMFSzD8O2U).\n",
    "\n",
    "*Note: You can safely ignore the warning about unparseable args shown after running the cell below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mAF9w7RTZU7c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--dataflow_endpoint DATAFLOW_ENDPOINT]\n",
      "                             [--project PROJECT] [--job_name JOB_NAME]\n",
      "                             [--staging_location STAGING_LOCATION]\n",
      "                             [--temp_location TEMP_LOCATION] [--region REGION]\n",
      "                             [--service_account_email SERVICE_ACCOUNT_EMAIL]\n",
      "                             [--no_auth]\n",
      "                             [--template_location TEMPLATE_LOCATION]\n",
      "                             [--label LABELS] [--update]\n",
      "                             [--transform_name_mapping TRANSFORM_NAME_MAPPING]\n",
      "                             [--enable_streaming_engine]\n",
      "                             [--dataflow_kms_key DATAFLOW_KMS_KEY]\n",
      "                             [--create_from_snapshot CREATE_FROM_SNAPSHOT]\n",
      "                             [--flexrs_goal {COST_OPTIMIZED,SPEED_OPTIMIZED}]\n",
      "                             [--dataflow_service_option DATAFLOW_SERVICE_OPTIONS]\n",
      "                             [--enable_hot_key_logging]\n",
      "                             [--enable_artifact_caching]\n",
      "                             [--impersonate_service_account IMPERSONATE_SERVICE_ACCOUNT]\n",
      "ipykernel_launcher.py: error: argument --flexrs_goal: invalid choice: 'c:\\\\Users\\\\mmdmo\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v2-1548021uRtOQF28lC.json' (choose from 'COST_OPTIMIZED', 'SPEED_OPTIMIZED')\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\argparse.py\", line 1787, in parse_known_args\n",
      "    namespace, args = self._parse_known_args(args, namespace)\n",
      "  File \"c:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\argparse.py\", line 1993, in _parse_known_args\n",
      "    start_index = consume_optional(start_index)\n",
      "  File \"c:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\argparse.py\", line 1933, in consume_optional\n",
      "    take_action(action, args, option_string)\n",
      "  File \"c:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\argparse.py\", line 1845, in take_action\n",
      "    argument_values = self._get_values(action, argument_strings)\n",
      "  File \"c:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\argparse.py\", line 2377, in _get_values\n",
      "    self._check_value(action, value)\n",
      "  File \"c:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\argparse.py\", line 2433, in _check_value\n",
      "    raise ArgumentError(action, msg % args)\n",
      "argparse.ArgumentError: argument --flexrs_goal: invalid choice: 'c:\\\\Users\\\\mmdmo\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v2-1548021uRtOQF28lC.json' (choose from 'COST_OPTIMIZED', 'SPEED_OPTIMIZED')\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\mmdmo\\AppData\\Local\\Temp\\ipykernel_15592\\3292164665.py\", line 12, in <module>\n",
      "    preprocessing_fn)\n",
      "  File \"c:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\apache_beam\\transforms\\ptransform.py\", line 590, in __ror__\n",
      "    p = pipeline.Pipeline('DirectRunner', PipelineOptions(sys.argv))\n",
      "  File \"c:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\apache_beam\\pipeline.py\", line 198, in __init__\n",
      "    errors = PipelineOptionsValidator(self._options, runner).validate()\n",
      "  File \"c:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\apache_beam\\options\\pipeline_options_validator.py\", line 146, in validate\n",
      "    errors.extend(self.options.view_as(cls).validate(self))\n",
      "  File \"c:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\apache_beam\\options\\pipeline_options.py\", line 388, in view_as\n",
      "    view = cls(self._flags)\n",
      "  File \"c:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\apache_beam\\options\\pipeline_options.py\", line 217, in __init__\n",
      "    self._visible_options, _ = parser.parse_known_args(flags)\n",
      "  File \"c:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\argparse.py\", line 1794, in parse_known_args\n",
      "    self.error(str(err))\n",
      "  File \"c:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\apache_beam\\options\\pipeline_options.py\", line 133, in error\n",
      "    super().error(message)\n",
      "  File \"c:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\argparse.py\", line 2508, in error\n",
      "    self.exit(2, _('%(prog)s: error: %(message)s\\n') % args)\n",
      "  File \"c:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\argparse.py\", line 2495, in exit\n",
      "    _sys.exit(status)\n",
      "SystemExit: 2\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"c:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"c:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "AttributeError: 'tuple' object has no attribute 'tb_frame'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\argparse.py\u001b[0m in \u001b[0;36mparse_known_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1786\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1787\u001b[1;33m             \u001b[0mnamespace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_known_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1788\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_UNRECOGNIZED_ARGS_ATTR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\argparse.py\u001b[0m in \u001b[0;36m_parse_known_args\u001b[1;34m(self, arg_strings, namespace)\u001b[0m\n\u001b[0;32m   1992\u001b[0m             \u001b[1;31m# consume the next optional and any arguments for it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1993\u001b[1;33m             \u001b[0mstart_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconsume_optional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1994\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\argparse.py\u001b[0m in \u001b[0;36mconsume_optional\u001b[1;34m(start_index)\u001b[0m\n\u001b[0;32m   1932\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moption_string\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maction_tuples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1933\u001b[1;33m                 \u001b[0mtake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moption_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1934\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\argparse.py\u001b[0m in \u001b[0;36mtake_action\u001b[1;34m(action, argument_strings, option_string)\u001b[0m\n\u001b[0;32m   1844\u001b[0m             \u001b[0mseen_actions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1845\u001b[1;33m             \u001b[0margument_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margument_strings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1846\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\argparse.py\u001b[0m in \u001b[0;36m_get_values\u001b[1;34m(self, action, arg_strings)\u001b[0m\n\u001b[0;32m   2376\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2377\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\argparse.py\u001b[0m in \u001b[0;36m_check_value\u001b[1;34m(self, action, value)\u001b[0m\n\u001b[0;32m   2432\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'invalid choice: %(value)r (choose from %(choices)s)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2433\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mArgumentError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mArgumentError\u001b[0m: argument --flexrs_goal: invalid choice: 'c:\\\\Users\\\\mmdmo\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v2-1548021uRtOQF28lC.json' (choose from 'COST_OPTIMIZED', 'SPEED_OPTIMIZED')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15592\\3292164665.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         (raw_data, raw_data_metadata) | tft_beam.AnalyzeAndTransformDataset(\n\u001b[1;32m---> 12\u001b[1;33m             preprocessing_fn)\n\u001b[0m\u001b[0;32m     13\u001b[0m     )\n",
      "\u001b[1;32mc:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\apache_beam\\transforms\\ptransform.py\u001b[0m in \u001b[0;36m__ror__\u001b[1;34m(self, left, label)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;31m# pylint: enable=wrong-import-order, wrong-import-position\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m       \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DirectRunner'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPipelineOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\apache_beam\\pipeline.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, runner, options, argv)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;31m# Validate pipeline options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m     \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipelineOptionsValidator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\apache_beam\\options\\pipeline_options_validator.py\u001b[0m in \u001b[0;36mvalidate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    145\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;34m'validate'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'validate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\apache_beam\\options\\pipeline_options.py\u001b[0m in \u001b[0;36mview_as\u001b[1;34m(self, cls)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \"\"\"\n\u001b[1;32m--> 388\u001b[1;33m     \u001b[0mview\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\apache_beam\\options\\pipeline_options.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, flags, **kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[1;31m# by the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_visible_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\argparse.py\u001b[0m in \u001b[0;36mparse_known_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1793\u001b[0m             \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1794\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\apache_beam\\options\\pipeline_options.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m    132\u001b[0m       \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\argparse.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m   2507\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'prog'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'message'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2508\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\argparse.py\u001b[0m in \u001b[0;36mexit\u001b[1;34m(self, status, message)\u001b[0m\n\u001b[0;32m   2494\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2495\u001b[1;33m         \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2069\u001b[0m                            'the full traceback.\\n']\n\u001b[0;32m   2070\u001b[0m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[1;32m-> 2071\u001b[1;33m                                                                      value))\n\u001b[0m\u001b[0;32m   2072\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2073\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \"\"\"\n\u001b[1;32m--> 754\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n\u001b[0;32m    632\u001b[0m                 \u001b[1;33m+\u001b[0m \u001b[0mchained_exception_message\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m                 + out_list)\n\u001b[0m\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[1;32m-> 1368\u001b[1;33m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[1;32m-> 1268\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1269\u001b[0m             )\n\u001b[0;32m   1270\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Minimal'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[1;32m-> 1125\u001b[1;33m                                                                tb_offset)\n\u001b[0m\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[1;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mmdmo\\anaconda3\\envs\\kaggle-nlp\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# Ignore the warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# a temporary directory is needed when analyzing the data\n",
    "with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
    "    \n",
    "    # define the pipeline using Apache Beam syntax\n",
    "    transformed_dataset, transform_fn = (\n",
    "        \n",
    "        # analyze and transform the dataset using the preprocessing function\n",
    "        (raw_data, raw_data_metadata) | tft_beam.AnalyzeAndTransformDataset(\n",
    "            preprocessing_fn)\n",
    "    )\n",
    "\n",
    "# unpack the transformed dataset\n",
    "transformed_data, transformed_metadata = transformed_dataset\n",
    "\n",
    "# print the results\n",
    "print('\\nRaw data:\\n{}\\n'.format(pprint.pformat(raw_data)))\n",
    "print('Transformed data:\\n{}'.format(pprint.pformat(transformed_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NO6LyTneNndy"
   },
   "source": [
    "## Is this the right answer?\n",
    "Previously, you used `tf.Transform` to do this:\n",
    "```\n",
    "x_centered = x - tft.mean(x)\n",
    "y_normalized = tft.scale_to_0_1(y)\n",
    "s_integerized = tft.compute_and_apply_vocabulary(s)\n",
    "x_centered_times_y_normalized = (x_centered * y_normalized)\n",
    "```\n",
    "#### x_centered\n",
    "With input of `[1, 2, 3]` the mean of `x` is 2, and you subtract it from `x` to center your `x` values at 0.  So the result of `[-1.0, 0.0, 1.0]` is correct.\n",
    "#### y_normalized\n",
    "Next, you scaled your `y` values between 0 and 1.  Your input was `[1, 2, 3]` so the result of `[0.0, 0.5, 1.0]` is correct.\n",
    "#### s_integerized\n",
    "You mapped your strings to indexes in a vocabulary, and there were only 2 words in your vocabulary (\"hello\" and \"world\").  So with input of `[\"hello\", \"world\", \"hello\"]` the result of `[0, 1, 0]` is correct.\n",
    "#### x_centered_times_y_normalized\n",
    "You created a new feature by crossing `x_centered` and `y_normalized` using multiplication.  Note that this multiplies the results, not the original values, and the new result of `[-0.0, 0.0, 1.0]` is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap Up\n",
    "\n",
    "In this lab, you went through the fundamentals of using Tensorflow Transform to turn raw data into features. This code can be used to transform both the training and serving data. However, the code can be quite complex if you'll be using this as a standalone library to build a pipeline (see this [notebook](https://www.tensorflow.org/tfx/tutorials/transform/census) for reference). Now that you know what is going on under the hood, you can use a higher-level set of tools like [Tensorflow Extended](https://www.tensorflow.org/tfx) to simplify the process. This will abstract some of the steps you did here like manually defining schemas and using `tft_beam` functions. It will also leverage other libraries, such as TFDV, to perform other processes in the usual machine learning pipeline like detecting anomalies. You will get to see these in the next lab."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "tghWegsjhpkt"
   ],
   "name": "L1_TFX_Transform.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1wmV2p-B2wcWKOTOmImfj6OZgVxWQgbp-",
     "timestamp": 1601072576594
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('kaggle-nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec5c1df393950c85460aac6c5f0d47d5950fe7c815f51de48caa93a009ae0140"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
